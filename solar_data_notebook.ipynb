{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('SolarPrediction.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get max value of Radiation\n",
    "max_radiation = df['Radiation'].max()\n",
    "print(max_radiation)\n",
    "#get min value of Radiation\n",
    "min_radiation = df['Radiation'].min()\n",
    "print(min_radiation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Rename 'Data' column to 'DateTime' for clarity\n",
    "df = df.rename(columns={'Data': 'DateTime'})\n",
    "\n",
    "# Convert DateTime to pandas datetime\n",
    "df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "\n",
    "# Convert Time column to datetime.time objects for better handling\n",
    "df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S').dt.time\n",
    "\n",
    "# Similarly, convert sunrise and sunset times\n",
    "df['TimeSunRise'] = pd.to_datetime(df['TimeSunRise'], format='%H:%M:%S').dt.time\n",
    "df['TimeSunSet'] = pd.to_datetime(df['TimeSunSet'], format='%H:%M:%S').dt.time\n",
    "\n",
    "# Sort by DateTime to ensure proper plotting\n",
    "df2 = df.sort_values('DateTime')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Alternative visualization: Create a combined plot for related measurements\n",
    "# Create a figure for related measurements (grouped by type)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Temperature and Humidity - often related\n",
    "ax1 = axes[0]\n",
    "ax1.set_title('Temperature and Humidity Over Time')\n",
    "ax1_twin = ax1.twinx()  # Create a twin axis for Humidity\n",
    "\n",
    "# Plot Temperature on left axis\n",
    "sns.lineplot(x='DateTime', y='Temperature', data=df, marker='o', color='red', label='Temperature', ax=ax1)\n",
    "ax1.set_ylabel('Temperature (°F)', color='red')\n",
    "ax1.tick_params(axis='y', colors='red')\n",
    "\n",
    "# Plot Humidity on right axis\n",
    "sns.lineplot(x='DateTime', y='Humidity', data=df, marker='o', color='blue', label='Humidity', ax=ax1_twin)\n",
    "ax1_twin.set_ylabel('Humidity (%)', color='blue')\n",
    "ax1_twin.tick_params(axis='y', colors='blue')\n",
    "\n",
    "# Add legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax1_twin.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "# Pressure and Radiation\n",
    "ax2 = axes[1]\n",
    "ax2.set_title('Pressure and Radiation Over Time')\n",
    "ax2_twin = ax2.twinx()  # Create a twin axis for Radiation\n",
    "\n",
    "# Plot Pressure on left axis\n",
    "sns.lineplot(x='DateTime', y='Pressure', data=df, marker='o', color='green', label='Pressure', ax=ax2)\n",
    "ax2.set_ylabel('Pressure (inHg)', color='green')\n",
    "ax2.tick_params(axis='y', colors='green')\n",
    "\n",
    "# Plot Radiation on right axis\n",
    "sns.lineplot(x='DateTime', y='Radiation', data=df, marker='o', color='orange', label='Radiation', ax=ax2_twin)\n",
    "ax2_twin.set_ylabel('Radiation', color='orange')\n",
    "ax2_twin.tick_params(axis='y', colors='orange')\n",
    "\n",
    "# Add legend\n",
    "lines1, labels1 = ax2.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2_twin.get_legend_handles_labels()\n",
    "ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "# Wind Speed and Direction\n",
    "ax3 = axes[2]\n",
    "ax3.set_title('Wind Speed and Direction Over Time')\n",
    "ax3_twin = ax3.twinx()  # Create a twin axis for Direction\n",
    "\n",
    "# Plot Speed on left axis\n",
    "sns.lineplot(x='DateTime', y='Speed', data=df, marker='o', color='purple', label='Speed', ax=ax3)\n",
    "ax3.set_ylabel('Wind Speed (mph)', color='purple')\n",
    "ax3.tick_params(axis='y', colors='purple')\n",
    "\n",
    "# Plot Direction on right axis\n",
    "sns.lineplot(x='DateTime', y='WindDirection(Degrees)', data=df, marker='o', color='teal', \n",
    "             label='Direction', ax=ax3_twin)\n",
    "ax3_twin.set_ylabel('Wind Direction (°)', color='teal')\n",
    "ax3_twin.tick_params(axis='y', colors='teal')\n",
    "\n",
    "# Add legend\n",
    "lines1, labels1 = ax3.get_legend_handles_labels()\n",
    "lines2, labels2 = ax3_twin.get_legend_handles_labels()\n",
    "ax3.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "\n",
    "# Format the x-axis with appropriate date formatting for all subplots\n",
    "plt.xticks(rotation=45)\n",
    "ax3.set_xlabel('Date and Time')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot correletions\n",
    "\n",
    "df['Hour'] = pd.to_datetime(df['Time'], format='%H:%M:%S').dt.hour\n",
    "df['Minute'] = pd.to_datetime(df['Time'], format='%H:%M:%S').dt.minute\n",
    "# Create a decimal hour (hour + minute/60) for better correlation\n",
    "df['DecimalHour'] = df['Hour'] + df['Minute']/60\n",
    "\n",
    "# Select only numeric columns for correlation analysis\n",
    "numeric_columns = ['UNIXTime', 'Radiation', 'Temperature', 'Pressure', \n",
    "                   'Humidity', 'WindDirection(Degrees)', 'Speed', \n",
    "                   'DecimalHour']\n",
    "corr_df = df[numeric_columns]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = corr_df.corr()\n",
    "\n",
    "# 1. Create a correlation heatmap\n",
    "plt.figure()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, \n",
    "            linewidths=0.5, fmt='.2f')\n",
    "plt.title('Correlation Matrix of Weather Measurements', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad = df['Radiation']\n",
    "\n",
    "plt.hist(rad, bins=100)\n",
    "plt.show()\n",
    "\n",
    "#log transform\n",
    "rad_log = np.log(rad)\n",
    "plt.hist(rad_log, bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lstm\n",
    "import data_prep\n",
    "\n",
    "target_col = 'Radiation'  # Column to predict\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, scalers, feature_cols, transform_info = data_prep.prepare_weather_data(\n",
    "        df,     \n",
    "        target_col='Radiation',  \n",
    "        window_size=12,\n",
    "        feature_selection_mode='basic',  # Start with basic features\n",
    "        standardize_features=True,#False,  # Try StandardScaler\n",
    "        use_solar_elevation=True,  # Try the solar elevation feature\n",
    "        use_piecewise_transform=False,  # Start without piecewise transform\n",
    "        log_transform=True, #figured out this one is super important\n",
    "        min_radiation_for_log=1,\n",
    "        #min_target_threshold=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import itertools\n",
    "from sklearn.metrics import r2_score\n",
    "device = torch.device('cpu')\n",
    "def objective(trial):\n",
    "    # Define hyperparameter space\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.1, 0.3, step=0.1)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-3, log=True)\n",
    "    clip_grad_norm = trial.suggest_categorical('clip_grad_norm', [1.0, 1.5])\n",
    "    value_multiplier = trial.suggest_categorical('value_multiplier', [0.1, 0.15, 0.2])\n",
    "    scheduler_type = trial.suggest_categorical('scheduler_type', ['plateau', 'cosine'])\n",
    "    loss_type = trial.suggest_categorical('loss_type', ['mse', 'value_aware'])\n",
    "    mse_weight = trial.suggest_categorical('mse_weight', [0.5, 0.7, 0.9])\n",
    "    mape_weight = trial.suggest_categorical('mape_weight', [0.5, 0.3, 0.1])\n",
    "    # Create model with trial parameters\n",
    "    model = lstm.WeatherLSTM(\n",
    "        input_dim=len(feature_cols),\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        output_dim=1,\n",
    "        dropout_prob=dropout_prob\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        epochs=5,  # Just 5 epochs for initial screening\n",
    "        batch_size=32,\n",
    "        learning_rate=learning_rate,  # Use the suggested learning rate\n",
    "        scheduler_type=scheduler_type,\n",
    "        loss_type=loss_type,\n",
    "        mse_weight=mse_weight,\n",
    "        mape_weight=mape_weight,\n",
    "        value_multiplier=value_multiplier,  # Use the suggested value multiplier\n",
    "        clip_grad_norm=clip_grad_norm  # Use the suggested clip gradient norm\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(torch.tensor(X_val, dtype=torch.float32).to(device))\n",
    "        predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    r2 = r2_score(y_val, predictions)\n",
    "    print(f\"Trial {trial.number}: hidden_dim={hidden_dim}, num_layers={num_layers}, \"\n",
    "          f\"dropout_prob={dropout_prob}, learning_rate={learning_rate:.6f}, \"\n",
    "          f\"clip_grad_norm={clip_grad_norm}, value_multiplier={value_multiplier}, R²={r2:.4f}\")\n",
    "    \n",
    "    return r2  # Return metric to maximize\n",
    "\n",
    "\n",
    "\n",
    "# Create study and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20, n_jobs=-1)\n",
    "\n",
    "# Get best parameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best value (R²): {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_dim = len(feature_cols)  # Number of features\n",
    "hidden_dim = 128  # Number of hidden units (increased from 64)\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_dim = 1  # Dimension of output (predicting a single value)\n",
    "mse_weight=0.7\n",
    "mape_weight=0.3\n",
    "dropout_prob =0.2\n",
    " \n",
    "# Initialize model\n",
    "device = torch.device('mps')\n",
    "model = lstm.WeatherLSTM(input_dim, hidden_dim, num_layers, output_dim, dropout_prob).to(device)\n",
    "\n",
    "# Store complete transform info in the model\n",
    "#model.transform_info = transform_info  # Store the whole transform_info object\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with advanced features\n",
    "model.fit(\n",
    "        X_train, y_train, X_val, y_val, \n",
    "        epochs=100, \n",
    "        batch_size=32, \n",
    "        learning_rate=0.0005, \n",
    "        patience=20,  # Increased patience for better convergence \n",
    "        device=device,\n",
    "        scheduler_type=\"cosine\",  # Try cosine annealing scheduler\n",
    "        weight_decay=0.001,  # L2 regularization\n",
    "        clip_grad_norm=1.5,  # Gradient clipping\n",
    "        loss_type='mse',#\"value_aware\",\n",
    "        mse_weight=mse_weight,\n",
    "        mape_weight=mape_weight,\n",
    "        value_multiplier=0.15 #adjust\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training metrics\n",
    "fig = model.plot_training_history()\n",
    "plt.show()\n",
    "model.transform_info = transform_info  # Store the whole transform_info object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('transform_info', transform_info)\n",
    "print('scalers', scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Evaluate on test data\n",
    "model_predictions_std_log_np, actuals_std_log_np, predictions_original_scale, actuals_original_scale, metrics = model.evaluate(\n",
    "        X_test, y_test,\n",
    "    target_scaler_object=scalers[\"Radiation_log\"],\n",
    "    transform_info_dict=transform_info,  # Pass the complete transform_info\n",
    "    )\n",
    "\n",
    "# Calculate RMSE in original scale\n",
    "rmse = np.sqrt(np.mean((predictions_original_scale - actuals_original_scale) ** 2))\n",
    "#print(f\"Test RMSE (original scale): {rmse:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot predictions\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actuals_original_scale, label='Actual')\n",
    "plt.plot(predictions_original_scale, label='Predicted')\n",
    "plt.title('Radiation Prediction - Test Set')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Radiation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actuals_original_scale[2700:3000], label='Actual')\n",
    "plt.plot(predictions_original_scale[2700:3000], label='Predicted')\n",
    "plt.title('Temperature Prediction - Test Set')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Radiation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot_utils as pu \n",
    "\n",
    "# You can create timestamps if data has a time component\n",
    "timestamps = df['UNIXTime'].values[-len(predictions_original_scale):]  # assuming UNIXTime exists in df\n",
    "\n",
    "# Make sure scaler is a dictionary with target_col as key\n",
    "if not isinstance(scalers, dict):\n",
    "    scalers_dict = {target_col: scalers}\n",
    "else:\n",
    "    scalers_dict = scalers\n",
    "\n",
    "fig_eval = pu.create_evaluation_dashboard(\n",
    "    predictions=predictions_original_scale, \n",
    "    actuals=actuals_original_scale, \n",
    "    scalers=None,  # !!! there is a bug here\n",
    "    target_col=target_col,\n",
    "    timestamps=timestamps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot histogram of y train\n",
    "plt.hist(y_train, bins=100)\n",
    "plt.show()\n",
    "\n",
    "#plot histogram of y val\n",
    "plt.hist(y_val, bins=100)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(y_test, bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Key Performance Metrics\n",
    "- **RMSE: 69.7374**\n",
    "- **Normalized RMSE: 0.0575** (excellent - below 0.1 indicates strong fit)\n",
    "- **MAE: 33.1875**\n",
    "- **MAPE: 37.98%**\n",
    "- **R²: 0.9310**\n",
    "- **Correlation Coefficient: 0.9656**\n",
    "\n",
    "## Error Distribution Analysis\n",
    "\n",
    "The error percentiles reveal a fascinating pattern:\n",
    "\n",
    "| Percentile | Error Value |\n",
    "|------------|-------------|\n",
    "| 25th       | 0.0217      |\n",
    "| 50th (Median) | 0.0817   |\n",
    "| 75th       | 46.7872     |\n",
    "| 90th       | 99.8823     |\n",
    "| 95th       | 143.1077    |\n",
    "| 99th       | 293.8402    |\n",
    "\n",
    "## Insights\n",
    "\n",
    "1. **Excellent Median Performance**: The median error of only 0.0817 indicates that at least 50% of the predictions are extremely accurate.\n",
    "\n",
    "2. **Right-Skewed Error Distribution**: The dramatic jump between the 50th percentile (0.0817) and the 75th percentile (46.7872) reveals a highly skewed error distribution.\n",
    "\n",
    "3. **Targeted Improvement Opportunity**: The large gap between percentiles suggests that the model is highly accurate for most cases but struggles significantly with a specific subset of observations.\n",
    "\n",
    "4. **Performance Dichotomy**: The excellent normalized RMSE (0.0575) and R² (0.93) alongside the higher MAPE (37.98%) confirm that the model generally performs well but likely struggles with smaller radiation values where small absolute errors translate to large percentage errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('weather_lstm_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Prediction LSTM Model: Technical Summary\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "**WeatherLSTM Network Structure:**<br>\n",
    "┌─────────────────────────────────────────────────────┐<br>\n",
    "│ ➤ LSTM Layer (16→256, layers=2, dropout=0.3)        │<br>\n",
    "│ ➤ Dropout Layer (p=0.3)                             │<br>\n",
    "│ ➤ Fully Connected Layer (256→128)                   │<br>\n",
    "│ ➤ ReLU Activation                                   │<br>\n",
    "│ ➤ Dropout Layer (p=0.3)                             │<br>\n",
    "│ ➤ Fully Connected Layer (128→64)                    │<br>\n",
    "│ ➤ ReLU Activation                                   │<br>\n",
    "│ ➤ Dropout Layer (p=0.3)                             │<br>\n",
    "│ ➤ Fully Connected Layer (64→1)                      │<br>\n",
    "└─────────────────────────────────────────────────────┘\n",
    "\n",
    "## Core Parameters\n",
    "- **Input Dimension**: 16 features\n",
    "- **Hidden Dimension**: 256 (increased from 64)\n",
    "- **LSTM Layers**: 2\n",
    "- **Output Dimension**: 1 (predicting a single value)\n",
    "- **Dropout Probability**: 0.3 (applied at multiple layers)\n",
    "- **Fully Connected Layers**: 256→128→64→1 with ReLU activations\n",
    "\n",
    "## Dataset Structure\n",
    "- **Training Set**: X_train shape: (19596, 24, 16), y_train shape: (19596, 1)\n",
    "- **Validation Set**: X_val shape: (6533, 24, 16), y_val shape: (6533, 1)\n",
    "- **Data Format**: 24 timesteps with 16 features per timestep\n",
    "\n",
    "## Loss Function\n",
    "The model uses a combined loss function:\n",
    "\n",
    "$\\mathcal{L}_{combined} = 0.7 \\times \\mathcal{L}_{MSE} + 0.3 \\times \\mathcal{L}_{MAPE}$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{L}_{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_{true} - y_{pred})^2$\n",
    "- $\\mathcal{L}_{MAPE} = \\frac{1}{n}\\sum_{i=1}^{n}\\left|\\frac{y_{true} - y_{pred}}{|y_{true}| + \\epsilon}\\right| \\times 100\\%$\n",
    "  - With $\\epsilon = 1.0$ to protect against division by zero\n",
    "\n",
    "## Regularization Techniques\n",
    "- **Dropout**: Probability of 0.3 at LSTM and after each fully connected layer\n",
    "- **L2 Regularization**: Weight decay of 0.0001\n",
    "- **Gradient Clipping**: Norm constrained to 1.0\n",
    "\n",
    "## Optimization Strategy\n",
    "- **Learning Rate Scheduler**: CosineAnnealingLR with T_max=100\n",
    "- **Patience for Early Stopping**: 15 epochs\n",
    "\n",
    "## Data Transformation\n",
    "- **Log Transformation**: Applied to Radiation values\n",
    "  - $Radiation_{log} = \\log(Radiation + \\epsilon)$\n",
    "  - Where $\\epsilon$ is a small constant (1e-06) to prevent log(0)\n",
    "- **Feature Engineering**: Added 'Radiation_is_low' binary feature (threshold: 1.2)\n",
    "\n",
    "## Input Features\n",
    "16 features including:\n",
    "- Raw measurements: Radiation, Temperature, Pressure, Humidity, Wind Direction, Speed\n",
    "- Engineered features: Radiation_is_low, SunriseMinutes, SunsetMinutes, DaylightMinutes\n",
    "- Temporal features: TimeSinceSunrise, TimeUntilSunset, DaylightPosition\n",
    "- Cyclical time encoding: TimeMinutesSin, TimeMinutesCos, HourOfDay\n",
    "\n",
    "This architecture optimally balances model complexity with regularization to prevent overfitting while capturing the temporal patterns in weather data for accurate radiation prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
